{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of software is bound by The MIT License (MIT)\n",
    "# Copyright (c) 2014 Siddharth Agrawal\n",
    "# Code written by : Siddharth Agrawal\n",
    "# Email ID : siddharth.950@gmail.com\n",
    "## https://github.com/siddharth-agrawal/Softmax-Regression/blob/master/softmaxRegression.py\n",
    "\n",
    "\n",
    "# Dhankar.Rohit@gmail.com - has modified the code to be used with various other data sets . \n",
    "\n",
    "\n",
    "import struct\n",
    "import numpy\n",
    "import array\n",
    "import time\n",
    "import scipy.sparse\n",
    "import scipy.optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        # Weight Decay -- http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression#Weight_Decay\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        # Numpy Random Number Generator - https://goo.gl/kNW46r\n",
    "        # It returns a  numpy RandomState object == rand\n",
    "        # \n",
    "                \n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "        \n",
    "        #\n",
    "        \n",
    "        #######################################################################################\n",
    "        \"\"\" Returns the groundtruth matrix for a set of labels \"\"\"\n",
    "        \n",
    "    def getGroundTruth(self, labels):\n",
    "    \n",
    "        \"\"\" Prepare data needed to construct groundtruth matrix \"\"\"\n",
    "    \n",
    "        labels = numpy.array(labels).flatten()\n",
    "        data   = numpy.ones(len(labels))\n",
    "        indptr = numpy.arange(len(labels)+1)\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = numpy.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    \n",
    "        #######################################################################################\n",
    "    \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = self.getGroundTruth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "    \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "            \n",
    "    def softmaxPredict(self, theta, input):\n",
    "    \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "    \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = numpy.zeros((input.shape[1], 1))\n",
    "        predictions[:, 0] = numpy.argmax(probabilities, axis = 0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    ###########################################################################################\n",
    "\"\"\" Loads the images from the provided file name \"\"\"\n",
    "\n",
    "    def loadMNISTImages(file_name):\n",
    "\n",
    "        \"\"\" Open the file \"\"\"\n",
    "\n",
    "        image_file = open(file_name, 'rb')\n",
    "\n",
    "        \"\"\" Read header information from the file \"\"\"\n",
    "\n",
    "        head1 = image_file.read(4)\n",
    "        head2 = image_file.read(4)\n",
    "        head3 = image_file.read(4)\n",
    "        head4 = image_file.read(4)\n",
    "\n",
    "        \"\"\" Format the header information for useful data \"\"\"\n",
    "\n",
    "        num_examples = struct.unpack('>I', head2)[0]\n",
    "        num_rows     = struct.unpack('>I', head3)[0]\n",
    "        num_cols     = struct.unpack('>I', head4)[0]\n",
    "\n",
    "        \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "\n",
    "        dataset = numpy.zeros((num_rows*num_cols, num_examples))\n",
    "\n",
    "        \"\"\" Read the actual image data \"\"\"\n",
    "\n",
    "        images_raw  = array.array('B', image_file.read())\n",
    "        image_file.close()\n",
    "\n",
    "        \"\"\" Arrange the data in columns \"\"\"\n",
    "\n",
    "        for i in range(num_examples):\n",
    "\n",
    "            limit1 = num_rows * num_cols * i\n",
    "            limit2 = num_rows * num_cols * (i + 1)\n",
    "\n",
    "            dataset[:, i] = images_raw[limit1 : limit2]\n",
    "\n",
    "        \"\"\" Normalize and return the dataset \"\"\"    \n",
    "\n",
    "        return dataset / 255\n",
    "    \n",
    "    \n",
    "    ###########################################################################################\n",
    "    \"\"\" Loads the image labels from the provided file name \"\"\"\n",
    "\n",
    "    def loadMNISTLabels(file_name):\n",
    "\n",
    "        \"\"\" Open the file \"\"\"\n",
    "\n",
    "        label_file = open(file_name, 'rb')\n",
    "\n",
    "        \"\"\" Read header information from the file \"\"\"\n",
    "\n",
    "        head1 = label_file.read(4)\n",
    "        head2 = label_file.read(4)\n",
    "\n",
    "        \"\"\" Format the header information for useful data \"\"\"\n",
    "\n",
    "        num_examples = struct.unpack('>I', head2)[0]\n",
    "\n",
    "        \"\"\" Initialize data labels as array of zeros \"\"\"\n",
    "\n",
    "        labels = numpy.zeros((num_examples, 1), dtype = numpy.int)\n",
    "\n",
    "        \"\"\" Read the label data \"\"\"\n",
    "\n",
    "        labels_raw = array.array('b', label_file.read())\n",
    "        label_file.close()\n",
    "\n",
    "        \"\"\" Copy and return the label data \"\"\"\n",
    "\n",
    "        labels[:, 0] = labels_raw[:]\n",
    "\n",
    "        return labels\n",
    "\n",
    "    \n",
    "    ###########################################################################################\n",
    "    \"\"\" Loads data, trains the model and predicts classes for test data \"\"\"\n",
    "\n",
    "    def executeSoftmaxRegression():\n",
    "\n",
    "        \"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "\n",
    "        input_size     = 784    # input vector size\n",
    "        num_classes    = 10     # number of classes\n",
    "        lamda          = 0.0001 # weight decay parameter\n",
    "        max_iterations = 100    # number of optimization iterations\n",
    "\n",
    "        \"\"\" Load MNIST training images and labels \"\"\"\n",
    "\n",
    "        training_data   = loadMNISTImages('train-images.idx3-ubyte')\n",
    "        training_labels = loadMNISTLabels('train-labels.idx1-ubyte')\n",
    "\n",
    "        \"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "\n",
    "        regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "\n",
    "        \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "\n",
    "        opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                                args = (training_data, training_labels,), method = 'L-BFGS-B', \n",
    "                                                jac = True, options = {'maxiter': max_iterations})\n",
    "        opt_theta     = opt_solution.x\n",
    "\n",
    "        \"\"\" Load MNIST test images and labels \"\"\"\n",
    "\n",
    "        test_data   = loadMNISTImages('t10k-images.idx3-ubyte') \n",
    "        test_labels = loadMNISTLabels('t10k-labels.idx1-ubyte')\n",
    "\n",
    "        \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "\n",
    "        predictions = regressor.softmaxPredict(opt_theta, test_data)\n",
    "\n",
    "        \"\"\" Print accuracy of the trained model \"\"\"\n",
    "\n",
    "        correct = test_labels[:, 0] == predictions[:, 0]\n",
    "        print \"\"\"Accuracy :\"\"\", numpy.mean(correct)\n",
    "    \n",
    "executeSoftmaxRegression()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Random Number Generator - https://goo.gl/kNW46r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-6-bbb31984a471>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-bbb31984a471>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def getGroundTruth(self, labels):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
